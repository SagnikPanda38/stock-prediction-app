{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = 'dragonwarrior38'\n",
    "os.environ['KAGGLE_KEY'] = 'adcd1264c62515c5f7b97a237cf6555'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59474a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dji_df=pd.read_csv(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\kaggle\\archive\\Processed_DJI.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666981c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=dji_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90865095",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330011b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dji_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop non-numeric or irrelevant columns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m drop_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m dji_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mdrop_cols)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict next day's Close → shift -1\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dji_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Drop non-numeric or irrelevant columns\n",
    "drop_cols = ['Date', 'Name', 'Source']\n",
    "df = dji_df.drop(columns=drop_cols)\n",
    "\n",
    "# Predict next day's Close → shift -1\n",
    "df['Target'] = df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['Target'])\n",
    "y = df['Target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f257eac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dji_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dji_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dji_df' is not defined"
     ]
    }
   ],
   "source": [
    "dji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636dbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dji_df=pd.read_csv(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\kaggle\\archive\\Processed_DJI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7597b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mom</th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>ROC_5</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>ROC_15</th>\n",
       "      <th>...</th>\n",
       "      <th>NZD</th>\n",
       "      <th>silver-F</th>\n",
       "      <th>RUSSELL-F</th>\n",
       "      <th>S&amp;P-F</th>\n",
       "      <th>CHF</th>\n",
       "      <th>Dollar index-F</th>\n",
       "      <th>Dollar index</th>\n",
       "      <th>wheat-F</th>\n",
       "      <th>XAG</th>\n",
       "      <th>XAU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>10428.049805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>10583.959961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.52</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>10572.019531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>10573.679688</td>\n",
       "      <td>0.515598</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>10606.860352</td>\n",
       "      <td>9.776045</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>23461.939453</td>\n",
       "      <td>6.511740</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>-0.230990</td>\n",
       "      <td>0.261016</td>\n",
       "      <td>1.290420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>23422.210938</td>\n",
       "      <td>-0.991838</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>-0.496952</td>\n",
       "      <td>-0.051116</td>\n",
       "      <td>0.401138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>23439.699219</td>\n",
       "      <td>-65.347705</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>-0.461690</td>\n",
       "      <td>0.389567</td>\n",
       "      <td>0.712119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>23409.470703</td>\n",
       "      <td>-1.387911</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>-0.627237</td>\n",
       "      <td>0.137871</td>\n",
       "      <td>-0.137742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>23271.279297</td>\n",
       "      <td>3.612171</td>\n",
       "      <td>-0.005903</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-1.239552</td>\n",
       "      <td>-0.698658</td>\n",
       "      <td>-0.249391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date         Close     Volume       mom      mom1      mom2  \\\n",
       "0     2009-12-31  10428.049805        NaN       NaN       NaN       NaN   \n",
       "1     2010-01-04  10583.959961        NaN  0.014951       NaN       NaN   \n",
       "2     2010-01-05  10572.019531        NaN -0.001128  0.014951       NaN   \n",
       "3     2010-01-06  10573.679688   0.515598  0.000157 -0.001128  0.014951   \n",
       "4     2010-01-07  10606.860352   9.776045  0.003138  0.000157 -0.001128   \n",
       "...          ...           ...        ...       ...       ...       ...   \n",
       "1979  2017-11-09  23461.939453   6.511740 -0.004304  0.000260  0.000374   \n",
       "1980  2017-11-10  23422.210938  -0.991838 -0.001693 -0.004304  0.000260   \n",
       "1981  2017-11-13  23439.699219 -65.347705  0.000747 -0.001693 -0.004304   \n",
       "1982  2017-11-14  23409.470703  -1.387911 -0.001290  0.000747 -0.001693   \n",
       "1983  2017-11-15  23271.279297   3.612171 -0.005903 -0.001290  0.000747   \n",
       "\n",
       "          mom3     ROC_5    ROC_10    ROC_15  ...   NZD  silver-F  RUSSELL-F  \\\n",
       "0          NaN       NaN       NaN       NaN  ...  0.03      0.26      -1.08   \n",
       "1          NaN       NaN       NaN       NaN  ...  1.52      3.26       1.61   \n",
       "2          NaN       NaN       NaN       NaN  ... -0.07      1.96      -0.20   \n",
       "3          NaN       NaN       NaN       NaN  ...  0.56      2.15      -0.02   \n",
       "4     0.014951       NaN       NaN       NaN  ... -0.72      0.94       0.50   \n",
       "...        ...       ...       ...       ...  ...   ...       ...        ...   \n",
       "1979  0.000392 -0.230990  0.261016  1.290420  ... -0.24     -0.62      -0.34   \n",
       "1980  0.000374 -0.496952 -0.051116  0.401138  ... -0.27     -0.58      -0.20   \n",
       "1981  0.000260 -0.461690  0.389567  0.712119  ... -0.38      0.72      -0.04   \n",
       "1982 -0.004304 -0.627237  0.137871 -0.137742  ... -0.39      0.17      -0.21   \n",
       "1983 -0.001693 -1.239552 -0.698658 -0.249391  ...  0.03     -0.60      -0.46   \n",
       "\n",
       "      S&P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  \n",
       "0     -1.00 -0.11           -0.08         -0.06    -0.48  0.30  0.39  \n",
       "1      1.62 -0.57           -0.59         -0.42     3.12  3.91  2.10  \n",
       "2      0.31  0.43            0.03          0.12    -0.90  1.42 -0.12  \n",
       "3      0.07 -0.56           -0.24         -0.17     2.62  2.25  1.77  \n",
       "4      0.40  0.58            0.58          0.54    -1.85  0.22 -0.58  \n",
       "...     ...   ...             ...           ...      ...   ...   ...  \n",
       "1979  -0.27 -0.61           -0.44         -0.45     0.53 -0.26  0.32  \n",
       "1980  -0.17  0.18           -0.07         -0.05     0.70 -0.71 -0.80  \n",
       "1981   0.10  0.06            0.12          0.11    -1.85  0.83  0.16  \n",
       "1982  -0.15 -0.70           -0.71         -0.70     1.00  0.01  0.24  \n",
       "1983  -0.50 -0.11            0.01         -0.02    -2.04 -0.22 -0.11  \n",
       "\n",
       "[1984 rows x 84 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e9d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric or irrelevant columns\n",
    "dji_df = dji_df.drop(columns=  ['Date', 'Name'])\n",
    "\n",
    "# Predict next day's Close → shift -1\n",
    "dji_df['Target'] = dji_df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = dji_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaac799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02276028 0.67367022 0.35458231 ... 0.46262957 0.41979337 0.30124224]\n",
      " [0.03414128 0.67353689 0.65455703 ... 0.74086197 0.77759652 0.64596273]\n",
      " [0.03753761 0.67290954 0.56085971 ... 0.43480633 0.46764546 0.40062112]\n",
      " ...\n",
      " [0.99677613 0.67330679 0.5029502  ... 0.63611566 0.788472   0.67494824]\n",
      " [0.97876625 0.67282272 0.41695055 ... 0.37206765 0.71071234 0.64803313]\n",
      " [0.9946794  0.67320111 0.53493206 ... 0.466994   0.78520935 0.64285714]]\n",
      "[11107.969727 11146.570313 11169.459961 ... 21844.009766 22024.869141\n",
      " 21750.730469]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "\n",
    "\n",
    "# Convert to dataframe if needed\n",
    "import numpy as np\n",
    "X_scaled = np.array(X_scaled)\n",
    "y = np.array(y)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe4b5278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mom</th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>ROC_5</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>ROC_15</th>\n",
       "      <th>...</th>\n",
       "      <th>silver-F</th>\n",
       "      <th>RUSSELL-F</th>\n",
       "      <th>S&amp;P-F</th>\n",
       "      <th>CHF</th>\n",
       "      <th>Dollar index-F</th>\n",
       "      <th>Dollar index</th>\n",
       "      <th>wheat-F</th>\n",
       "      <th>XAG</th>\n",
       "      <th>XAU</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-10-19</td>\n",
       "      <td>10978.620117</td>\n",
       "      <td>0.062238</td>\n",
       "      <td>-0.014813</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>-0.002865</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.379118</td>\n",
       "      <td>0.309742</td>\n",
       "      <td>1.109587</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>-1.74</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.62</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>-4.14</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>11107.969727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-10-20</td>\n",
       "      <td>11107.969727</td>\n",
       "      <td>-0.216333</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>-0.014813</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>-0.002865</td>\n",
       "      <td>0.107152</td>\n",
       "      <td>1.279393</td>\n",
       "      <td>2.516681</td>\n",
       "      <td>...</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.82</td>\n",
       "      <td>11146.570313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-10-21</td>\n",
       "      <td>11146.570313</td>\n",
       "      <td>-1.527112</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>-0.014813</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.468698</td>\n",
       "      <td>1.808364</td>\n",
       "      <td>3.323312</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-3.09</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>11132.559570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-10-26</td>\n",
       "      <td>11169.459961</td>\n",
       "      <td>-0.559593</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.001257</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>1.738286</td>\n",
       "      <td>1.352579</td>\n",
       "      <td>2.053412</td>\n",
       "      <td>...</td>\n",
       "      <td>2.42</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>11126.280273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-10-27</td>\n",
       "      <td>11126.280273</td>\n",
       "      <td>0.779029</td>\n",
       "      <td>-0.003866</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.001257</td>\n",
       "      <td>0.164842</td>\n",
       "      <td>0.272170</td>\n",
       "      <td>1.446343</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.97</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>11113.950195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>22048.699219</td>\n",
       "      <td>-0.697099</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>-0.001496</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>0.147432</td>\n",
       "      <td>1.555383</td>\n",
       "      <td>1.885097</td>\n",
       "      <td>...</td>\n",
       "      <td>2.90</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.10</td>\n",
       "      <td>21844.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>21844.009766</td>\n",
       "      <td>-1.708505</td>\n",
       "      <td>-0.009284</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>-0.001496</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>-0.826700</td>\n",
       "      <td>0.217736</td>\n",
       "      <td>1.074555</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-4.24</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.84</td>\n",
       "      <td>21858.320313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>22024.869141</td>\n",
       "      <td>-0.917905</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.108079</td>\n",
       "      <td>0.039193</td>\n",
       "      <td>1.445623</td>\n",
       "      <td>...</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.79</td>\n",
       "      <td>21750.730469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>21750.730469</td>\n",
       "      <td>10.102563</td>\n",
       "      <td>-0.012447</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>-0.427025</td>\n",
       "      <td>-1.250195</td>\n",
       "      <td>-0.210218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.43</td>\n",
       "      <td>21674.509766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>21899.890625</td>\n",
       "      <td>-0.988463</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>-0.012447</td>\n",
       "      <td>-0.450473</td>\n",
       "      <td>-0.839694</td>\n",
       "      <td>-0.291520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>21812.089844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1114 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date         Close     Volume       mom      mom1      mom2  \\\n",
       "0     2010-10-19  10978.620117   0.062238 -0.014813  0.007314 -0.002865   \n",
       "1     2010-10-20  11107.969727  -0.216333  0.011782 -0.014813  0.007314   \n",
       "2     2010-10-21  11146.570313  -1.527112  0.003475  0.011782 -0.014813   \n",
       "3     2010-10-26  11169.459961  -0.559593  0.000485  0.002829 -0.001257   \n",
       "4     2010-10-27  11126.280273   0.779029 -0.003866  0.000485  0.002829   \n",
       "...          ...           ...        ...       ...       ...       ...   \n",
       "1109  2017-08-09  22048.699219  -0.697099 -0.001659 -0.001496  0.001159   \n",
       "1110  2017-08-10  21844.009766  -1.708505 -0.009284 -0.001659 -0.001496   \n",
       "1111  2017-08-16  22024.869141  -0.917905  0.001176  0.000240  0.006194   \n",
       "1112  2017-08-17  21750.730469  10.102563 -0.012447  0.001176  0.000240   \n",
       "1113  2017-08-22  21899.890625  -0.988463  0.009037  0.001349 -0.003504   \n",
       "\n",
       "          mom3     ROC_5    ROC_10    ROC_15  ...  silver-F  RUSSELL-F  S&P-F  \\\n",
       "0    -0.000136 -0.379118  0.309742  1.109587  ...     -2.59      -1.74  -1.23   \n",
       "1    -0.002865  0.107152  1.279393  2.516681  ...      1.03       0.91   0.94   \n",
       "2     0.007314  0.468698  1.808364  3.323312  ...     -3.69      -0.53   0.09   \n",
       "3     0.003475  1.738286  1.352579  2.053412  ...      2.42      -0.23   0.00   \n",
       "4    -0.001257  0.164842  0.272170  1.446343  ...     -2.97      -0.26  -0.34   \n",
       "...        ...       ...       ...       ...  ...       ...        ...    ...   \n",
       "1109  0.003029  0.147432  1.555383  1.885097  ...      2.90      -0.86   0.01   \n",
       "1110  0.001159 -0.826700  0.217736  1.074555  ...      1.20      -1.97  -1.44   \n",
       "1111  0.000655 -0.108079  0.039193  1.445623  ...      1.21       0.07   0.15   \n",
       "1112  0.006194 -0.427025 -1.250195 -0.210218  ...      0.67      -2.00  -1.54   \n",
       "1113 -0.012447 -0.450473 -0.839694 -0.291520  ...     -0.19       1.14   1.02   \n",
       "\n",
       "       CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU        Target  \n",
       "0     1.27            1.66          1.62    -2.58 -4.14 -2.51  11107.969727  \n",
       "1    -1.01           -1.29         -1.29     2.52  2.44  0.82  11146.570313  \n",
       "2     0.64            0.28          0.32    -3.09 -3.26 -1.55  11132.559570  \n",
       "3     1.17            0.79          0.79     3.65  0.85 -0.04  11126.280273  \n",
       "4     0.47            0.60          0.57     0.61 -1.22 -1.09  11113.950195  \n",
       "...    ...             ...           ...      ...   ...   ...           ...  \n",
       "1109 -1.10           -0.10         -0.11     0.60  2.64  1.10  21844.009766  \n",
       "1110 -0.11           -0.14         -0.16    -4.24  1.21  0.84  21858.320313  \n",
       "1111 -0.70           -0.34         -0.33    -2.50  2.58  0.79  21750.730469  \n",
       "1112 -0.31            0.10          0.09    -1.25 -0.30  0.43  21674.509766  \n",
       "1113  0.67            0.48          0.49    -1.71 -0.38 -0.65  21812.089844  \n",
       "\n",
       "[1114 rows x 85 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ea3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input shape: (1052, 60, 82)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 60\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(sequence_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-sequence_length:i])\n",
    "    y_seq.append(y[i])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)  # Should be (samples, 60, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1a9bcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02276028, 0.67367022, 0.35458231, ..., 0.46262957, 0.41979337,\n",
       "        0.30124224],\n",
       "       [0.03414128, 0.67353689, 0.65455703, ..., 0.74086197, 0.77759652,\n",
       "        0.64596273],\n",
       "       [0.03753761, 0.67290954, 0.56085971, ..., 0.43480633, 0.46764546,\n",
       "        0.40062112],\n",
       "       ...,\n",
       "       [0.99677613, 0.67330679, 0.5029502 , ..., 0.63611566, 0.788472  ,\n",
       "        0.67494824],\n",
       "       [0.97876625, 0.67282272, 0.41695055, ..., 0.37206765, 0.71071234,\n",
       "        0.64803313],\n",
       "       [0.9946794 , 0.67320111, 0.53493206, ..., 0.466994  , 0.78520935,\n",
       "        0.64285714]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "456c774b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.02276028, 0.67367022, 0.35458231, ..., 0.46262957,\n",
       "         0.41979337, 0.30124224],\n",
       "        [0.03414128, 0.67353689, 0.65455703, ..., 0.74086197,\n",
       "         0.77759652, 0.64596273],\n",
       "        [0.03753761, 0.67290954, 0.56085971, ..., 0.43480633,\n",
       "         0.46764546, 0.40062112],\n",
       "        ...,\n",
       "        [0.11173295, 0.67328286, 0.52579438, ..., 0.54118931,\n",
       "         0.50299076, 0.29606625],\n",
       "        [0.09711579, 0.71526799, 0.36537667, ..., 0.45826514,\n",
       "         0.862969  , 0.7494824 ],\n",
       "        [0.10311906, 0.67316812, 0.58675204, ..., 0.69612657,\n",
       "         0.65851006, 0.53623188]],\n",
       "\n",
       "       [[0.03414128, 0.67353689, 0.65455703, ..., 0.74086197,\n",
       "         0.77759652, 0.64596273],\n",
       "        [0.03753761, 0.67290954, 0.56085971, ..., 0.43480633,\n",
       "         0.46764546, 0.40062112],\n",
       "        [0.03955158, 0.6733726 , 0.52712937, ..., 0.80250955,\n",
       "         0.69113649, 0.55693582],\n",
       "        ...,\n",
       "        [0.09711579, 0.71526799, 0.36537667, ..., 0.45826514,\n",
       "         0.862969  , 0.7494824 ],\n",
       "        [0.10311906, 0.67316812, 0.58675204, ..., 0.69612657,\n",
       "         0.65851006, 0.53623188],\n",
       "        [0.11616131, 0.67209734, 0.66225907, ..., 0.58210584,\n",
       "         0.73626971, 0.626294  ]],\n",
       "\n",
       "       [[0.03753761, 0.67290954, 0.56085971, ..., 0.43480633,\n",
       "         0.46764546, 0.40062112],\n",
       "        [0.03955158, 0.6733726 , 0.52712937, ..., 0.80250955,\n",
       "         0.69113649, 0.55693582],\n",
       "        [0.03575236, 0.67401329, 0.47805849, ..., 0.63666121,\n",
       "         0.57857531, 0.44824017],\n",
       "        ...,\n",
       "        [0.10311906, 0.67316812, 0.58675204, ..., 0.69612657,\n",
       "         0.65851006, 0.53623188],\n",
       "        [0.11616131, 0.67209734, 0.66225907, ..., 0.58210584,\n",
       "         0.73626971, 0.626294  ],\n",
       "        [0.13280216, 0.67306714, 0.51189544, ..., 0.43807965,\n",
       "         0.65035345, 0.55383023]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.90048744, 0.66742354, 0.49407859, ..., 0.65739225,\n",
       "         0.58129418, 0.49068323],\n",
       "        [0.8978848 , 0.67330665, 0.50574076, ..., 0.57392253,\n",
       "         0.5622621 , 0.47412008],\n",
       "        [0.89181116, 0.67100299, 0.48445327, ..., 0.49863612,\n",
       "         0.57041871, 0.49171843],\n",
       "        ...,\n",
       "        [0.99392018, 0.67561314, 0.54853206, ..., 0.59738134,\n",
       "         0.60413268, 0.52380952],\n",
       "        [0.99478767, 0.67912043, 0.52671449, ..., 0.56192035,\n",
       "         0.69004894, 0.61076605],\n",
       "        [1.        , 0.67166638, 0.50479391, ..., 0.52373159,\n",
       "         0.71669386, 0.59627329]],\n",
       "\n",
       "       [[0.8978848 , 0.67330665, 0.50574076, ..., 0.57392253,\n",
       "         0.5622621 , 0.47412008],\n",
       "        [0.89181116, 0.67100299, 0.48445327, ..., 0.49863612,\n",
       "         0.57041871, 0.49171843],\n",
       "        [0.89202752, 0.6726306 , 0.5229932 , ..., 0.55755592,\n",
       "         0.54105492, 0.50310559],\n",
       "        ...,\n",
       "        [0.99478767, 0.67912043, 0.52671449, ..., 0.56192035,\n",
       "         0.69004894, 0.61076605],\n",
       "        [1.        , 0.67166638, 0.50479391, ..., 0.52373159,\n",
       "         0.71669386, 0.59627329],\n",
       "        [0.99677613, 0.67330679, 0.5029502 , ..., 0.63611566,\n",
       "         0.788472  , 0.67494824]],\n",
       "\n",
       "       [[0.89181116, 0.67100299, 0.48445327, ..., 0.49863612,\n",
       "         0.57041871, 0.49171843],\n",
       "        [0.89202752, 0.6726306 , 0.5229932 , ..., 0.55755592,\n",
       "         0.54105492, 0.50310559],\n",
       "        [0.89596852, 0.67393966, 0.54588482, ..., 0.58210584,\n",
       "         0.68026101, 0.5931677 ],\n",
       "        ...,\n",
       "        [1.        , 0.67166638, 0.50479391, ..., 0.52373159,\n",
       "         0.71669386, 0.59627329],\n",
       "        [0.99677613, 0.67330679, 0.5029502 , ..., 0.63611566,\n",
       "         0.788472  , 0.67494824],\n",
       "        [0.97876625, 0.67282272, 0.41695055, ..., 0.37206765,\n",
       "         0.71071234, 0.64803313]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08fa3ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12229.290039, 12273.259766, 12268.19043 , ..., 21844.009766,\n",
       "       22024.869141, 21750.730469])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfc1666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40644e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 231278688.0000 - val_loss: 384868000.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 232340672.0000 - val_loss: 384744256.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 226867568.0000 - val_loss: 384673024.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 234970720.0000 - val_loss: 384609344.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 232924816.0000 - val_loss: 384548672.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 233479200.0000 - val_loss: 384489632.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 233612240.0000 - val_loss: 384431456.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 229535696.0000 - val_loss: 384374304.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 232280544.0000 - val_loss: 384317728.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 228670688.0000 - val_loss: 384261216.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 235806736.0000 - val_loss: 384205152.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 228882976.0000 - val_loss: 384149568.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 232186560.0000 - val_loss: 384093984.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 235072064.0000 - val_loss: 384038464.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 231359200.0000 - val_loss: 383983296.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 231115968.0000 - val_loss: 383928224.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 232134720.0000 - val_loss: 383873248.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 232558608.0000 - val_loss: 383818496.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 231838336.0000 - val_loss: 383763680.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 230344672.0000 - val_loss: 383709216.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 229208000.0000 - val_loss: 383654528.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 232379312.0000 - val_loss: 383600160.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 230593296.0000 - val_loss: 383545856.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 229631872.0000 - val_loss: 383491520.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 231527696.0000 - val_loss: 383437184.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 229530784.0000 - val_loss: 383383040.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 226494720.0000 - val_loss: 383328864.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 232763792.0000 - val_loss: 383274368.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 231390416.0000 - val_loss: 383220448.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 231805792.0000 - val_loss: 383166528.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 229443888.0000 - val_loss: 383112352.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 230486656.0000 - val_loss: 383058272.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 228997584.0000 - val_loss: 383004352.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 228984032.0000 - val_loss: 382950272.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 230674816.0000 - val_loss: 382896320.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 229850048.0000 - val_loss: 382842432.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 230105936.0000 - val_loss: 382788512.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 229613216.0000 - val_loss: 382734656.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 232647440.0000 - val_loss: 382680896.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 229084928.0000 - val_loss: 382626912.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 227655616.0000 - val_loss: 382573184.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 228528080.0000 - val_loss: 382519360.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 229994160.0000 - val_loss: 382465760.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 232337168.0000 - val_loss: 382411776.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 231980576.0000 - val_loss: 382358272.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 225545568.0000 - val_loss: 382304800.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 230916336.0000 - val_loss: 382251104.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 227983456.0000 - val_loss: 382197408.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 230116736.0000 - val_loss: 382143872.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 231305248.0000 - val_loss: 382090304.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1))  # Output layer (next day's close price)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fca6bba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6db2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Optionally inverse scale (if you're saving prices for actual display)\n",
    "# y_pred_actual = scaler.inverse_transform(...) — only needed if Close was scaled separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77e9a992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77.12731 ],\n",
       "       [77.12732 ],\n",
       "       [77.12731 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12731 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12731 ],\n",
       "       [77.12732 ],\n",
       "       [77.12731 ],\n",
       "       [77.12732 ],\n",
       "       [77.12731 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12731 ],\n",
       "       [77.12731 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12731 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.12731 ],\n",
       "       [77.127304],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12731 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.127304],\n",
       "       [77.127304],\n",
       "       [77.12731 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12729 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12733 ],\n",
       "       [77.12733 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ],\n",
       "       [77.12732 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "534754f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"lstm_stock_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cac6ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.4))  # ← increase this\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18567ac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must call `compile()` before using the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m      3\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      5\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), callbacks\u001b[38;5;241m=\u001b[39m[early_stop])\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:1049\u001b[0m, in \u001b[0;36mTrainer._assert_compile_called\u001b[1;34m(self, method_name)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1048\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You must call `compile()` before using the model."
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history=model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "          validation_data=(X_test, y_test), callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "041ccd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 189ms/step - loss: 231348832.0000 - mae: 15047.6191 - val_loss: 384851776.0000 - val_mae: 19571.2012\n",
      "Epoch 2/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 234276720.0000 - mae: 15146.0732 - val_loss: 384773760.0000 - val_mae: 19569.2090\n",
      "Epoch 3/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 231888576.0000 - mae: 15070.1064 - val_loss: 384728224.0000 - val_mae: 19568.0449\n",
      "Epoch 4/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 232196256.0000 - mae: 15084.9658 - val_loss: 384687936.0000 - val_mae: 19567.0156\n",
      "Epoch 5/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - loss: 233219440.0000 - mae: 15109.3359 - val_loss: 384649536.0000 - val_mae: 19566.0352\n",
      "Epoch 6/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - loss: 232815920.0000 - mae: 15100.4248 - val_loss: 384612064.0000 - val_mae: 19565.0762\n",
      "Epoch 7/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 228307168.0000 - mae: 14947.6299 - val_loss: 384575200.0000 - val_mae: 19564.1348\n",
      "Epoch 8/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - loss: 230977872.0000 - mae: 15033.4551 - val_loss: 384538432.0000 - val_mae: 19563.1973\n",
      "Epoch 9/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 233811728.0000 - mae: 15136.1934 - val_loss: 384502016.0000 - val_mae: 19562.2637\n",
      "Epoch 10/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 232234192.0000 - mae: 15078.6689 - val_loss: 384465856.0000 - val_mae: 19561.3398\n",
      "Epoch 11/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 232157520.0000 - mae: 15075.7236 - val_loss: 384430048.0000 - val_mae: 19560.4258\n",
      "Epoch 12/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 232610544.0000 - mae: 15103.1445 - val_loss: 384394336.0000 - val_mae: 19559.5117\n",
      "Epoch 13/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - loss: 231339856.0000 - mae: 15047.9248 - val_loss: 384358560.0000 - val_mae: 19558.5977\n",
      "Epoch 14/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - loss: 238760544.0000 - mae: 15291.7627 - val_loss: 384322880.0000 - val_mae: 19557.6855\n",
      "Epoch 15/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - loss: 233106624.0000 - mae: 15103.6904 - val_loss: 384287456.0000 - val_mae: 19556.7793\n",
      "Epoch 16/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 228904128.0000 - mae: 14968.8496 - val_loss: 384252256.0000 - val_mae: 19555.8789\n",
      "Epoch 17/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 232664832.0000 - mae: 15092.2080 - val_loss: 384216864.0000 - val_mae: 19554.9746\n",
      "Epoch 18/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 233169056.0000 - mae: 15120.0107 - val_loss: 384181536.0000 - val_mae: 19554.0742\n",
      "Epoch 19/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 227914240.0000 - mae: 14930.1709 - val_loss: 384146432.0000 - val_mae: 19553.1738\n",
      "Epoch 20/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 232066640.0000 - mae: 15071.6924 - val_loss: 384110912.0000 - val_mae: 19552.2656\n",
      "Epoch 21/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 231685840.0000 - mae: 15055.4766 - val_loss: 384075680.0000 - val_mae: 19551.3633\n",
      "Epoch 22/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 233946928.0000 - mae: 15137.7842 - val_loss: 384040384.0000 - val_mae: 19550.4629\n",
      "Epoch 23/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 233017584.0000 - mae: 15102.2842 - val_loss: 384005408.0000 - val_mae: 19549.5664\n",
      "Epoch 24/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 230668928.0000 - mae: 15026.1953 - val_loss: 383970208.0000 - val_mae: 19548.6680\n",
      "Epoch 25/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 142ms/step - loss: 227001344.0000 - mae: 14907.3633 - val_loss: 383935264.0000 - val_mae: 19547.7754\n",
      "Epoch 26/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - loss: 229630032.0000 - mae: 14992.4824 - val_loss: 383900288.0000 - val_mae: 19546.8789\n",
      "Epoch 27/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - loss: 230946544.0000 - mae: 15037.4854 - val_loss: 383865408.0000 - val_mae: 19545.9863\n",
      "Epoch 28/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 234213920.0000 - mae: 15149.4609 - val_loss: 383830368.0000 - val_mae: 19545.0898\n",
      "Epoch 29/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 232865824.0000 - mae: 15101.3262 - val_loss: 383795360.0000 - val_mae: 19544.1973\n",
      "Epoch 30/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 236457552.0000 - mae: 15221.2822 - val_loss: 383760640.0000 - val_mae: 19543.3066\n",
      "Epoch 31/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 232687472.0000 - mae: 15094.1641 - val_loss: 383725728.0000 - val_mae: 19542.4141\n",
      "Epoch 32/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - loss: 229861312.0000 - mae: 15002.0977 - val_loss: 383690880.0000 - val_mae: 19541.5234\n",
      "Epoch 33/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 232443824.0000 - mae: 15091.1709 - val_loss: 383656000.0000 - val_mae: 19540.6289\n",
      "Epoch 34/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - loss: 233596416.0000 - mae: 15126.7939 - val_loss: 383621024.0000 - val_mae: 19539.7363\n",
      "Epoch 35/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 230531856.0000 - mae: 15024.5664 - val_loss: 383586240.0000 - val_mae: 19538.8457\n",
      "Epoch 36/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 231708896.0000 - mae: 15063.4033 - val_loss: 383551648.0000 - val_mae: 19537.9590\n",
      "Epoch 37/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 231765344.0000 - mae: 15066.4219 - val_loss: 383516640.0000 - val_mae: 19537.0645\n",
      "Epoch 38/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 130ms/step - loss: 229364336.0000 - mae: 14980.2842 - val_loss: 383482048.0000 - val_mae: 19536.1777\n",
      "Epoch 39/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 233014384.0000 - mae: 15105.0068 - val_loss: 383447104.0000 - val_mae: 19535.2852\n",
      "Epoch 40/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 233775504.0000 - mae: 15134.8535 - val_loss: 383412480.0000 - val_mae: 19534.3965\n",
      "Epoch 41/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 130ms/step - loss: 230689744.0000 - mae: 15032.3828 - val_loss: 383377792.0000 - val_mae: 19533.5098\n",
      "Epoch 42/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - loss: 234663616.0000 - mae: 15161.5977 - val_loss: 383343296.0000 - val_mae: 19532.6250\n",
      "Epoch 43/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 233036752.0000 - mae: 15105.7100 - val_loss: 383308544.0000 - val_mae: 19531.7363\n",
      "Epoch 44/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 230332400.0000 - mae: 15009.1182 - val_loss: 383273888.0000 - val_mae: 19530.8516\n",
      "Epoch 45/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 230702368.0000 - mae: 15021.4521 - val_loss: 383239072.0000 - val_mae: 19529.9570\n",
      "Epoch 46/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 231175296.0000 - mae: 15044.3232 - val_loss: 383204288.0000 - val_mae: 19529.0664\n",
      "Epoch 47/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 227727296.0000 - mae: 14923.5166 - val_loss: 383169568.0000 - val_mae: 19528.1777\n",
      "Epoch 48/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 231316384.0000 - mae: 15049.6328 - val_loss: 383134688.0000 - val_mae: 19527.2852\n",
      "Epoch 49/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 231477584.0000 - mae: 15059.0488 - val_loss: 383099904.0000 - val_mae: 19526.3945\n",
      "Epoch 50/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 231448384.0000 - mae: 15055.8496 - val_loss: 383065536.0000 - val_mae: 19525.5137\n",
      "Epoch 51/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 230832080.0000 - mae: 15037.5918 - val_loss: 383030784.0000 - val_mae: 19524.6250\n",
      "Epoch 52/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 233262656.0000 - mae: 15119.3076 - val_loss: 382995968.0000 - val_mae: 19523.7344\n",
      "Epoch 53/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - loss: 227790240.0000 - mae: 14933.5928 - val_loss: 382961568.0000 - val_mae: 19522.8535\n",
      "Epoch 54/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 227757888.0000 - mae: 14929.9961 - val_loss: 382927232.0000 - val_mae: 19521.9727\n",
      "Epoch 55/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 233575008.0000 - mae: 15121.9678 - val_loss: 382892640.0000 - val_mae: 19521.0859\n",
      "Epoch 56/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - loss: 230768048.0000 - mae: 15036.1230 - val_loss: 382857824.0000 - val_mae: 19520.1973\n",
      "Epoch 57/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - loss: 229774752.0000 - mae: 15001.3076 - val_loss: 382823232.0000 - val_mae: 19519.3086\n",
      "Epoch 58/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 233818912.0000 - mae: 15135.1182 - val_loss: 382788512.0000 - val_mae: 19518.4199\n",
      "Epoch 59/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 133ms/step - loss: 231634928.0000 - mae: 15057.6514 - val_loss: 382754240.0000 - val_mae: 19517.5410\n",
      "Epoch 60/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - loss: 229323296.0000 - mae: 14979.8154 - val_loss: 382719680.0000 - val_mae: 19516.6582\n",
      "Epoch 61/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 227751888.0000 - mae: 14924.6621 - val_loss: 382685248.0000 - val_mae: 19515.7754\n",
      "Epoch 62/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 229604864.0000 - mae: 14991.3701 - val_loss: 382650624.0000 - val_mae: 19514.8867\n",
      "Epoch 63/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 232025600.0000 - mae: 15069.0850 - val_loss: 382615776.0000 - val_mae: 19513.9961\n",
      "Epoch 64/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 232830464.0000 - mae: 15102.5078 - val_loss: 382581440.0000 - val_mae: 19513.1133\n",
      "Epoch 65/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 229416144.0000 - mae: 14989.0605 - val_loss: 382547136.0000 - val_mae: 19512.2363\n",
      "Epoch 66/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 228834032.0000 - mae: 14973.0723 - val_loss: 382512448.0000 - val_mae: 19511.3457\n",
      "Epoch 67/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 229764112.0000 - mae: 14999.9873 - val_loss: 382477984.0000 - val_mae: 19510.4629\n",
      "Epoch 68/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 228272208.0000 - mae: 14947.4873 - val_loss: 382443488.0000 - val_mae: 19509.5801\n",
      "Epoch 69/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 228203200.0000 - mae: 14939.2832 - val_loss: 382408768.0000 - val_mae: 19508.6895\n",
      "Epoch 70/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 228186240.0000 - mae: 14937.3721 - val_loss: 382374176.0000 - val_mae: 19507.8027\n",
      "Epoch 71/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - loss: 228284128.0000 - mae: 14951.4307 - val_loss: 382339584.0000 - val_mae: 19506.9180\n",
      "Epoch 72/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 234413360.0000 - mae: 15152.3838 - val_loss: 382305184.0000 - val_mae: 19506.0332\n",
      "Epoch 73/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - loss: 229167584.0000 - mae: 14979.0703 - val_loss: 382270816.0000 - val_mae: 19505.1543\n",
      "Epoch 74/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - loss: 228899568.0000 - mae: 14967.5098 - val_loss: 382236288.0000 - val_mae: 19504.2676\n",
      "Epoch 75/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - loss: 226271504.0000 - mae: 14879.9648 - val_loss: 382201760.0000 - val_mae: 19503.3848\n",
      "Epoch 76/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 228607120.0000 - mae: 14965.7178 - val_loss: 382167168.0000 - val_mae: 19502.4961\n",
      "Epoch 77/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 228866912.0000 - mae: 14962.6426 - val_loss: 382132704.0000 - val_mae: 19501.6113\n",
      "Epoch 78/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 142ms/step - loss: 232028528.0000 - mae: 15079.8564 - val_loss: 382098400.0000 - val_mae: 19500.7344\n",
      "Epoch 79/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 229438800.0000 - mae: 14992.6299 - val_loss: 382064128.0000 - val_mae: 19499.8535\n",
      "Epoch 80/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 230938688.0000 - mae: 15034.4707 - val_loss: 382029536.0000 - val_mae: 19498.9668\n",
      "Epoch 81/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 230474176.0000 - mae: 15019.3350 - val_loss: 381995104.0000 - val_mae: 19498.0840\n",
      "Epoch 82/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 229354896.0000 - mae: 14986.1562 - val_loss: 381960736.0000 - val_mae: 19497.2031\n",
      "Epoch 83/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 130ms/step - loss: 225391760.0000 - mae: 14848.3965 - val_loss: 381926272.0000 - val_mae: 19496.3184\n",
      "Epoch 84/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 230861968.0000 - mae: 15030.3916 - val_loss: 381891744.0000 - val_mae: 19495.4336\n",
      "Epoch 85/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - loss: 229649648.0000 - mae: 14994.2129 - val_loss: 381857376.0000 - val_mae: 19494.5527\n",
      "Epoch 86/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 229215328.0000 - mae: 14980.7861 - val_loss: 381822912.0000 - val_mae: 19493.6680\n",
      "Epoch 87/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 228810208.0000 - mae: 14964.6943 - val_loss: 381788352.0000 - val_mae: 19492.7812\n",
      "Epoch 88/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 230814240.0000 - mae: 15031.0771 - val_loss: 381753696.0000 - val_mae: 19491.8926\n",
      "Epoch 89/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 231542672.0000 - mae: 15054.6846 - val_loss: 381719296.0000 - val_mae: 19491.0098\n",
      "Epoch 90/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - loss: 228722352.0000 - mae: 14956.1533 - val_loss: 381685152.0000 - val_mae: 19490.1348\n",
      "Epoch 91/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 227660432.0000 - mae: 14927.8389 - val_loss: 381650944.0000 - val_mae: 19489.2559\n",
      "Epoch 92/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - loss: 229386672.0000 - mae: 14981.9375 - val_loss: 381616352.0000 - val_mae: 19488.3691\n",
      "Epoch 93/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 232463936.0000 - mae: 15088.1357 - val_loss: 381582016.0000 - val_mae: 19487.4883\n",
      "Epoch 94/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - loss: 228813568.0000 - mae: 14965.7354 - val_loss: 381547520.0000 - val_mae: 19486.6035\n",
      "Epoch 95/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 227017248.0000 - mae: 14900.8867 - val_loss: 381513216.0000 - val_mae: 19485.7227\n",
      "Epoch 96/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 229287760.0000 - mae: 14979.8760 - val_loss: 381478720.0000 - val_mae: 19484.8379\n",
      "Epoch 97/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 226858752.0000 - mae: 14898.4922 - val_loss: 381444416.0000 - val_mae: 19483.9570\n",
      "Epoch 98/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - loss: 225020624.0000 - mae: 14840.8633 - val_loss: 381410080.0000 - val_mae: 19483.0762\n",
      "Epoch 99/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - loss: 227184864.0000 - mae: 14911.6211 - val_loss: 381375680.0000 - val_mae: 19482.1934\n",
      "Epoch 100/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 226983648.0000 - mae: 14905.4463 - val_loss: 381341248.0000 - val_mae: 19481.3125\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 🔧 Compile first!\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 🛑 Add EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ✅ Now fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59676fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eec2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dji_df=pd.read_csv(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\kaggle\\archive\\Processed_DJI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d57f968",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Date', 'Name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dji_df \u001b[38;5;241m=\u001b[39m dji_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Drop the last row (its target is NaN)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Date', 'Name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "dji_df = dji_df.drop(columns=['Date', 'Name'])\n",
    "\n",
    "\n",
    "df['Target'] = dji_df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e1a9f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\frame.py:4485\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value, refs)\u001b[0m\n\u001b[0;32m   4484\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4485\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   4487\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Target'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict next day's Close → shift -1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Drop the last row (its target is NaN)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dji_df \u001b[38;5;241m=\u001b[39m dji_df\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\frame.py:4538\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4535\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   4536\u001b[0m             refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_mgr(key, value, refs)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\frame.py:4488\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value, refs)\u001b[0m\n\u001b[0;32m   4485\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   4487\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[1;32m-> 4488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis), key, value, refs)\n\u001b[0;32m   4489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iset_item_mgr(loc, value, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1385\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[1;34m(self, loc, item, value, refs)\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_update_mgr_locs(loc)\n\u001b[1;32m-> 1385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_update_blklocs_and_blknos(loc)\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m new_axis\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (block,)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1421\u001b[0m, in \u001b[0;36mBlockManager._insert_update_blklocs_and_blknos\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;66;03m# Accessing public blklocs ensures the public versions are initialized\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loc \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblklocs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;66;03m# np.append is a lot faster, let's use it if we can.\u001b[39;00m\n\u001b[1;32m-> 1421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blknos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blknos, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks))\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m loc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m# np.append is a lot faster, let's use it if we can.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\numpy\\lib\\function_base.py:5616\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   5615\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m-> 5616\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[0;32m   5617\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   5618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1871\u001b[0m, in \u001b[0;36mravel\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_ravel_dispatcher)\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mravel\u001b[39m(a, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a contiguous flattened array.\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \n\u001b[0;32m   1772\u001b[0m \u001b[38;5;124;03m    A 1-D array, containing the elements of the input, is returned.  A copy is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1869\u001b[0m \n\u001b[0;32m   1870\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, np\u001b[38;5;241m.\u001b[39mmatrix):\n\u001b[0;32m   1872\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m asarray(a)\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m   1873\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'matrix'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Predict next day's Close → shift -1\n",
    "dji_df['Target'] = dji_df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = dji_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d32b578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mom</th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>ROC_5</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>ROC_15</th>\n",
       "      <th>ROC_20</th>\n",
       "      <th>...</th>\n",
       "      <th>NZD</th>\n",
       "      <th>silver-F</th>\n",
       "      <th>RUSSELL-F</th>\n",
       "      <th>S&amp;P-F</th>\n",
       "      <th>CHF</th>\n",
       "      <th>Dollar index-F</th>\n",
       "      <th>Dollar index</th>\n",
       "      <th>wheat-F</th>\n",
       "      <th>XAG</th>\n",
       "      <th>XAU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10428.049805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10583.959961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.52</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10572.019531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10573.679688</td>\n",
       "      <td>0.515598</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10606.860352</td>\n",
       "      <td>9.776045</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>23461.939453</td>\n",
       "      <td>6.511740</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>-0.230990</td>\n",
       "      <td>0.261016</td>\n",
       "      <td>1.290420</td>\n",
       "      <td>2.718486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>23422.210938</td>\n",
       "      <td>-0.991838</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>-0.496952</td>\n",
       "      <td>-0.051116</td>\n",
       "      <td>0.401138</td>\n",
       "      <td>2.406860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>23439.699219</td>\n",
       "      <td>-65.347705</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>-0.461690</td>\n",
       "      <td>0.389567</td>\n",
       "      <td>0.712119</td>\n",
       "      <td>2.102797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>23409.470703</td>\n",
       "      <td>-1.387911</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>-0.627237</td>\n",
       "      <td>0.137871</td>\n",
       "      <td>-0.137742</td>\n",
       "      <td>1.791640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>23271.279297</td>\n",
       "      <td>3.612171</td>\n",
       "      <td>-0.005903</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-1.239552</td>\n",
       "      <td>-0.698658</td>\n",
       "      <td>-0.249391</td>\n",
       "      <td>0.490896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close     Volume       mom      mom1      mom2      mom3  \\\n",
       "0     10428.049805        NaN       NaN       NaN       NaN       NaN   \n",
       "1     10583.959961        NaN  0.014951       NaN       NaN       NaN   \n",
       "2     10572.019531        NaN -0.001128  0.014951       NaN       NaN   \n",
       "3     10573.679688   0.515598  0.000157 -0.001128  0.014951       NaN   \n",
       "4     10606.860352   9.776045  0.003138  0.000157 -0.001128  0.014951   \n",
       "...            ...        ...       ...       ...       ...       ...   \n",
       "1979  23461.939453   6.511740 -0.004304  0.000260  0.000374  0.000392   \n",
       "1980  23422.210938  -0.991838 -0.001693 -0.004304  0.000260  0.000374   \n",
       "1981  23439.699219 -65.347705  0.000747 -0.001693 -0.004304  0.000260   \n",
       "1982  23409.470703  -1.387911 -0.001290  0.000747 -0.001693 -0.004304   \n",
       "1983  23271.279297   3.612171 -0.005903 -0.001290  0.000747 -0.001693   \n",
       "\n",
       "         ROC_5    ROC_10    ROC_15    ROC_20  ...   NZD  silver-F  RUSSELL-F  \\\n",
       "0          NaN       NaN       NaN       NaN  ...  0.03      0.26      -1.08   \n",
       "1          NaN       NaN       NaN       NaN  ...  1.52      3.26       1.61   \n",
       "2          NaN       NaN       NaN       NaN  ... -0.07      1.96      -0.20   \n",
       "3          NaN       NaN       NaN       NaN  ...  0.56      2.15      -0.02   \n",
       "4          NaN       NaN       NaN       NaN  ... -0.72      0.94       0.50   \n",
       "...        ...       ...       ...       ...  ...   ...       ...        ...   \n",
       "1979 -0.230990  0.261016  1.290420  2.718486  ... -0.24     -0.62      -0.34   \n",
       "1980 -0.496952 -0.051116  0.401138  2.406860  ... -0.27     -0.58      -0.20   \n",
       "1981 -0.461690  0.389567  0.712119  2.102797  ... -0.38      0.72      -0.04   \n",
       "1982 -0.627237  0.137871 -0.137742  1.791640  ... -0.39      0.17      -0.21   \n",
       "1983 -1.239552 -0.698658 -0.249391  0.490896  ...  0.03     -0.60      -0.46   \n",
       "\n",
       "      S&P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  \n",
       "0     -1.00 -0.11           -0.08         -0.06    -0.48  0.30  0.39  \n",
       "1      1.62 -0.57           -0.59         -0.42     3.12  3.91  2.10  \n",
       "2      0.31  0.43            0.03          0.12    -0.90  1.42 -0.12  \n",
       "3      0.07 -0.56           -0.24         -0.17     2.62  2.25  1.77  \n",
       "4      0.40  0.58            0.58          0.54    -1.85  0.22 -0.58  \n",
       "...     ...   ...             ...           ...      ...   ...   ...  \n",
       "1979  -0.27 -0.61           -0.44         -0.45     0.53 -0.26  0.32  \n",
       "1980  -0.17  0.18           -0.07         -0.05     0.70 -0.71 -0.80  \n",
       "1981   0.10  0.06            0.12          0.11    -1.85  0.83  0.16  \n",
       "1982  -0.15 -0.70           -0.71         -0.70     1.00  0.01  0.24  \n",
       "1983  -0.50 -0.11            0.01         -0.02    -2.04 -0.22 -0.11  \n",
       "\n",
       "[1984 rows x 82 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a423d651",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Date', 'Name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop non-numeric or irrelevant columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dji_df \u001b[38;5;241m=\u001b[39m dji_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m  [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predict next day's Close → shift -1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Date', 'Name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop non-numeric or irrelevant columns\n",
    "dji_df = dji_df.drop(columns=  ['Date', 'Name'])\n",
    "\n",
    "# Predict next day's Close → shift -1\n",
    "dji_df['Target'] = dji_df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = dji_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b4d91c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Date', 'Name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop non-numeric or irrelevant columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dji_df \u001b[38;5;241m=\u001b[39m dji_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m  [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predict next day's Close → shift -1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Date', 'Name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop non-numeric or irrelevant columns\n",
    "dji_df = dji_df.drop(columns=  ['Date', 'Name'])\n",
    "\n",
    "# Predict next day's Close → shift -1\n",
    "dji_df['Target'] = dji_df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = dji_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d303fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dji_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dji_dfdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dji_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Example: next day price as target\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Drop the last row (its target is NaN)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dji_df \u001b[38;5;241m=\u001b[39m dji_df\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dji_df' is not defined"
     ]
    }
   ],
   "source": [
    "dji_dfdf['Target'] = dji_df['Close'].shift(-1)  # Example: next day price as target\n",
    "\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = dji_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d95ddece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mom</th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>ROC_5</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>ROC_15</th>\n",
       "      <th>ROC_20</th>\n",
       "      <th>...</th>\n",
       "      <th>NZD</th>\n",
       "      <th>silver-F</th>\n",
       "      <th>RUSSELL-F</th>\n",
       "      <th>S&amp;P-F</th>\n",
       "      <th>CHF</th>\n",
       "      <th>Dollar index-F</th>\n",
       "      <th>Dollar index</th>\n",
       "      <th>wheat-F</th>\n",
       "      <th>XAG</th>\n",
       "      <th>XAU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10428.049805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10583.959961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.52</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10572.019531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10573.679688</td>\n",
       "      <td>0.515598</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10606.860352</td>\n",
       "      <td>9.776045</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>0.014951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>23461.939453</td>\n",
       "      <td>6.511740</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>-0.230990</td>\n",
       "      <td>0.261016</td>\n",
       "      <td>1.290420</td>\n",
       "      <td>2.718486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>23422.210938</td>\n",
       "      <td>-0.991838</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>-0.496952</td>\n",
       "      <td>-0.051116</td>\n",
       "      <td>0.401138</td>\n",
       "      <td>2.406860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>23439.699219</td>\n",
       "      <td>-65.347705</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>-0.461690</td>\n",
       "      <td>0.389567</td>\n",
       "      <td>0.712119</td>\n",
       "      <td>2.102797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>23409.470703</td>\n",
       "      <td>-1.387911</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>-0.627237</td>\n",
       "      <td>0.137871</td>\n",
       "      <td>-0.137742</td>\n",
       "      <td>1.791640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>23271.279297</td>\n",
       "      <td>3.612171</td>\n",
       "      <td>-0.005903</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>-1.239552</td>\n",
       "      <td>-0.698658</td>\n",
       "      <td>-0.249391</td>\n",
       "      <td>0.490896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close     Volume       mom      mom1      mom2      mom3  \\\n",
       "0     10428.049805        NaN       NaN       NaN       NaN       NaN   \n",
       "1     10583.959961        NaN  0.014951       NaN       NaN       NaN   \n",
       "2     10572.019531        NaN -0.001128  0.014951       NaN       NaN   \n",
       "3     10573.679688   0.515598  0.000157 -0.001128  0.014951       NaN   \n",
       "4     10606.860352   9.776045  0.003138  0.000157 -0.001128  0.014951   \n",
       "...            ...        ...       ...       ...       ...       ...   \n",
       "1979  23461.939453   6.511740 -0.004304  0.000260  0.000374  0.000392   \n",
       "1980  23422.210938  -0.991838 -0.001693 -0.004304  0.000260  0.000374   \n",
       "1981  23439.699219 -65.347705  0.000747 -0.001693 -0.004304  0.000260   \n",
       "1982  23409.470703  -1.387911 -0.001290  0.000747 -0.001693 -0.004304   \n",
       "1983  23271.279297   3.612171 -0.005903 -0.001290  0.000747 -0.001693   \n",
       "\n",
       "         ROC_5    ROC_10    ROC_15    ROC_20  ...   NZD  silver-F  RUSSELL-F  \\\n",
       "0          NaN       NaN       NaN       NaN  ...  0.03      0.26      -1.08   \n",
       "1          NaN       NaN       NaN       NaN  ...  1.52      3.26       1.61   \n",
       "2          NaN       NaN       NaN       NaN  ... -0.07      1.96      -0.20   \n",
       "3          NaN       NaN       NaN       NaN  ...  0.56      2.15      -0.02   \n",
       "4          NaN       NaN       NaN       NaN  ... -0.72      0.94       0.50   \n",
       "...        ...       ...       ...       ...  ...   ...       ...        ...   \n",
       "1979 -0.230990  0.261016  1.290420  2.718486  ... -0.24     -0.62      -0.34   \n",
       "1980 -0.496952 -0.051116  0.401138  2.406860  ... -0.27     -0.58      -0.20   \n",
       "1981 -0.461690  0.389567  0.712119  2.102797  ... -0.38      0.72      -0.04   \n",
       "1982 -0.627237  0.137871 -0.137742  1.791640  ... -0.39      0.17      -0.21   \n",
       "1983 -1.239552 -0.698658 -0.249391  0.490896  ...  0.03     -0.60      -0.46   \n",
       "\n",
       "      S&P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  \n",
       "0     -1.00 -0.11           -0.08         -0.06    -0.48  0.30  0.39  \n",
       "1      1.62 -0.57           -0.59         -0.42     3.12  3.91  2.10  \n",
       "2      0.31  0.43            0.03          0.12    -0.90  1.42 -0.12  \n",
       "3      0.07 -0.56           -0.24         -0.17     2.62  2.25  1.77  \n",
       "4      0.40  0.58            0.58          0.54    -1.85  0.22 -0.58  \n",
       "...     ...   ...             ...           ...      ...   ...   ...  \n",
       "1979  -0.27 -0.61           -0.44         -0.45     0.53 -0.26  0.32  \n",
       "1980  -0.17  0.18           -0.07         -0.05     0.70 -0.71 -0.80  \n",
       "1981   0.10  0.06            0.12          0.11    -1.85  0.83  0.16  \n",
       "1982  -0.15 -0.70           -0.71         -0.70     1.00  0.01  0.24  \n",
       "1983  -0.50 -0.11            0.01         -0.02    -2.04 -0.22 -0.11  \n",
       "\n",
       "[1984 rows x 82 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60aadffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      3\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m----> 4\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_scaled)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert to dataframe if needed\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "\n",
    "\n",
    "# Convert to dataframe if needed\n",
    "import numpy as np\n",
    "X_scaled = np.array(X_scaled)\n",
    "y = np.array(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c07f4dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dji_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dji_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dji_df' is not defined"
     ]
    }
   ],
   "source": [
    "dji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be1654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dji_df=pd.read_csv(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\kaggle\\archive\\Processed_DJI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea2584be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric or irrelevant columns\n",
    "dji_df = dji_df.drop(columns=  ['Date', 'Name'])\n",
    "\n",
    "# Predict next day's Close → shift -1\n",
    "dji_df['Target'] = dji_df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = dji_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7d5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02276028 0.67367022 0.35458231 ... 0.46262957 0.41979337 0.30124224]\n",
      " [0.03414128 0.67353689 0.65455703 ... 0.74086197 0.77759652 0.64596273]\n",
      " [0.03753761 0.67290954 0.56085971 ... 0.43480633 0.46764546 0.40062112]\n",
      " ...\n",
      " [0.9946794  0.67320111 0.53493206 ... 0.466994   0.78520935 0.64285714]\n",
      " [0.97055894 0.67847565 0.38127075 ... 0.53518822 0.6286025  0.60559006]\n",
      " [0.983683   0.67316734 0.62359749 ... 0.51009274 0.62425231 0.49378882]]\n",
      "[11107.969727 11146.570313 11132.55957  ... 21750.730469 21674.509766\n",
      " 21812.089844]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "\n",
    "\n",
    "# Convert to dataframe if needed\n",
    "import numpy as np\n",
    "X_scaled = np.array(X_scaled)\n",
    "y = np.array(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3fe68f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input shape: (1054, 60, 82)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 60\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(sequence_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-sequence_length:i])\n",
    "    y_seq.append(y[i])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)  # Should be (samples, 60, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5eb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afb7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 336ms/step - loss: 235618112.0000 - mae: 15195.8340 - val_loss: 386133952.0000 - val_mae: 19603.8633\n",
      "Epoch 2/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - loss: 231683296.0000 - mae: 15061.4062 - val_loss: 386048192.0000 - val_mae: 19601.6758\n",
      "Epoch 3/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 238ms/step - loss: 229075056.0000 - mae: 14975.5508 - val_loss: 386002432.0000 - val_mae: 19600.5078\n",
      "Epoch 4/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 234ms/step - loss: 230817072.0000 - mae: 15030.0908 - val_loss: 385963744.0000 - val_mae: 19599.5234\n",
      "Epoch 5/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 244ms/step - loss: 231551600.0000 - mae: 15059.6973 - val_loss: 385926560.0000 - val_mae: 19598.5742\n",
      "Epoch 6/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - loss: 232577840.0000 - mae: 15088.9316 - val_loss: 385887424.0000 - val_mae: 19597.5742\n",
      "Epoch 7/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 253ms/step - loss: 228549552.0000 - mae: 14951.7266 - val_loss: 385850080.0000 - val_mae: 19596.6211\n",
      "Epoch 8/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step - loss: 230646608.0000 - mae: 15022.6416 - val_loss: 385813600.0000 - val_mae: 19595.6914\n",
      "Epoch 9/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step - loss: 233809776.0000 - mae: 15132.1201 - val_loss: 385777600.0000 - val_mae: 19594.7734\n",
      "Epoch 10/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - loss: 230150768.0000 - mae: 15010.8535 - val_loss: 385741984.0000 - val_mae: 19593.8633\n",
      "Epoch 11/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 244ms/step - loss: 231900704.0000 - mae: 15068.2334 - val_loss: 385706624.0000 - val_mae: 19592.9629\n",
      "Epoch 12/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 240ms/step - loss: 232375856.0000 - mae: 15080.5273 - val_loss: 385670016.0000 - val_mae: 19592.0273\n",
      "Epoch 13/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 245ms/step - loss: 236534064.0000 - mae: 15225.6807 - val_loss: 385630944.0000 - val_mae: 19591.0312\n",
      "Epoch 14/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 272ms/step - loss: 230699488.0000 - mae: 15028.0430 - val_loss: 385593408.0000 - val_mae: 19590.0703\n",
      "Epoch 15/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 246ms/step - loss: 236836640.0000 - mae: 15240.7852 - val_loss: 385556064.0000 - val_mae: 19589.1191\n",
      "Epoch 16/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 238ms/step - loss: 234768992.0000 - mae: 15162.5146 - val_loss: 385519392.0000 - val_mae: 19588.1836\n",
      "Epoch 17/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 244ms/step - loss: 229981872.0000 - mae: 15005.0898 - val_loss: 385483104.0000 - val_mae: 19587.2559\n",
      "Epoch 18/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 238ms/step - loss: 229350832.0000 - mae: 14980.9365 - val_loss: 385446816.0000 - val_mae: 19586.3320\n",
      "Epoch 19/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 239ms/step - loss: 231475312.0000 - mae: 15053.9033 - val_loss: 385410816.0000 - val_mae: 19585.4102\n",
      "Epoch 20/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step - loss: 232600736.0000 - mae: 15092.8125 - val_loss: 385375008.0000 - val_mae: 19584.4961\n",
      "Epoch 21/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step - loss: 228751504.0000 - mae: 14959.7705 - val_loss: 385339360.0000 - val_mae: 19583.5898\n",
      "Epoch 22/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 231300896.0000 - mae: 15053.5127 - val_loss: 385303616.0000 - val_mae: 19582.6758\n",
      "Epoch 23/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 244ms/step - loss: 233895680.0000 - mae: 15143.0713 - val_loss: 385267904.0000 - val_mae: 19581.7617\n",
      "Epoch 24/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 245ms/step - loss: 230588080.0000 - mae: 15020.6621 - val_loss: 385232320.0000 - val_mae: 19580.8555\n",
      "Epoch 25/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 245ms/step - loss: 230682288.0000 - mae: 15027.3848 - val_loss: 385196832.0000 - val_mae: 19579.9492\n",
      "Epoch 26/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 230ms/step - loss: 230906960.0000 - mae: 15038.4775 - val_loss: 385161376.0000 - val_mae: 19579.0430\n",
      "Epoch 27/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 239ms/step - loss: 229352480.0000 - mae: 14987.2334 - val_loss: 385125888.0000 - val_mae: 19578.1367\n",
      "Epoch 28/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 243ms/step - loss: 234911200.0000 - mae: 15170.7314 - val_loss: 385090464.0000 - val_mae: 19577.2305\n",
      "Epoch 29/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 240ms/step - loss: 234526864.0000 - mae: 15157.8379 - val_loss: 385055296.0000 - val_mae: 19576.3320\n",
      "Epoch 30/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 240ms/step - loss: 230169744.0000 - mae: 15009.2168 - val_loss: 385019968.0000 - val_mae: 19575.4316\n",
      "Epoch 31/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 249ms/step - loss: 231757344.0000 - mae: 15065.2158 - val_loss: 384984736.0000 - val_mae: 19574.5312\n",
      "Epoch 32/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 239ms/step - loss: 229284064.0000 - mae: 14979.2158 - val_loss: 384949536.0000 - val_mae: 19573.6309\n",
      "Epoch 33/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 246ms/step - loss: 236297200.0000 - mae: 15218.7275 - val_loss: 384914208.0000 - val_mae: 19572.7305\n",
      "Epoch 34/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 251ms/step - loss: 231833856.0000 - mae: 15066.2568 - val_loss: 384879040.0000 - val_mae: 19571.8320\n",
      "Epoch 35/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 248ms/step - loss: 230265856.0000 - mae: 15016.4697 - val_loss: 384843776.0000 - val_mae: 19570.9316\n",
      "Epoch 36/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step - loss: 230701728.0000 - mae: 15029.7402 - val_loss: 384808512.0000 - val_mae: 19570.0312\n",
      "Epoch 37/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 261ms/step - loss: 233743216.0000 - mae: 15131.0488 - val_loss: 384773376.0000 - val_mae: 19569.1309\n",
      "Epoch 38/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 249ms/step - loss: 227884512.0000 - mae: 14935.2422 - val_loss: 384738528.0000 - val_mae: 19568.2402\n",
      "Epoch 39/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 246ms/step - loss: 233218144.0000 - mae: 15116.2432 - val_loss: 384703712.0000 - val_mae: 19567.3516\n",
      "Epoch 40/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 233ms/step - loss: 229894032.0000 - mae: 15002.0986 - val_loss: 384668768.0000 - val_mae: 19566.4570\n",
      "Epoch 41/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 251ms/step - loss: 233234096.0000 - mae: 15107.8428 - val_loss: 384633504.0000 - val_mae: 19565.5586\n",
      "Epoch 42/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 249ms/step - loss: 233372256.0000 - mae: 15121.4805 - val_loss: 384598720.0000 - val_mae: 19564.6680\n",
      "Epoch 43/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 248ms/step - loss: 235574128.0000 - mae: 15191.9023 - val_loss: 384563744.0000 - val_mae: 19563.7754\n",
      "Epoch 44/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 243ms/step - loss: 232552720.0000 - mae: 15093.9580 - val_loss: 384529152.0000 - val_mae: 19562.8906\n",
      "Epoch 45/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - loss: 232611360.0000 - mae: 15095.5586 - val_loss: 384494176.0000 - val_mae: 19561.9961\n",
      "Epoch 46/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 243ms/step - loss: 233446400.0000 - mae: 15122.7051 - val_loss: 384459104.0000 - val_mae: 19561.0996\n",
      "Epoch 47/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 246ms/step - loss: 235462128.0000 - mae: 15187.6025 - val_loss: 384424032.0000 - val_mae: 19560.2031\n",
      "Epoch 48/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step - loss: 226883840.0000 - mae: 14895.5205 - val_loss: 384389280.0000 - val_mae: 19559.3164\n",
      "Epoch 49/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - loss: 233306416.0000 - mae: 15108.7227 - val_loss: 384354304.0000 - val_mae: 19558.4219\n",
      "Epoch 50/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 243ms/step - loss: 230443008.0000 - mae: 15019.6152 - val_loss: 384319680.0000 - val_mae: 19557.5352\n",
      "Epoch 51/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 265ms/step - loss: 232279552.0000 - mae: 15081.0322 - val_loss: 384285088.0000 - val_mae: 19556.6523\n",
      "Epoch 52/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 251ms/step - loss: 230681408.0000 - mae: 15026.7012 - val_loss: 384250560.0000 - val_mae: 19555.7695\n",
      "Epoch 53/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 262ms/step - loss: 231263168.0000 - mae: 15049.9385 - val_loss: 384215520.0000 - val_mae: 19554.8711\n",
      "Epoch 54/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 249ms/step - loss: 224296016.0000 - mae: 14812.0869 - val_loss: 384180800.0000 - val_mae: 19553.9863\n",
      "Epoch 55/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 250ms/step - loss: 230601536.0000 - mae: 15024.0117 - val_loss: 384145792.0000 - val_mae: 19553.0898\n",
      "Epoch 56/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 248ms/step - loss: 231030320.0000 - mae: 15039.0039 - val_loss: 384111168.0000 - val_mae: 19552.2031\n",
      "Epoch 57/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 256ms/step - loss: 230460768.0000 - mae: 15017.7373 - val_loss: 384076416.0000 - val_mae: 19551.3164\n",
      "Epoch 58/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 257ms/step - loss: 234551504.0000 - mae: 15160.7803 - val_loss: 384041824.0000 - val_mae: 19550.4316\n",
      "Epoch 59/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 268ms/step - loss: 227402208.0000 - mae: 14916.2988 - val_loss: 384007296.0000 - val_mae: 19549.5469\n",
      "Epoch 60/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 275ms/step - loss: 233286400.0000 - mae: 15117.8447 - val_loss: 383972384.0000 - val_mae: 19548.6562\n",
      "Epoch 61/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 273ms/step - loss: 232786688.0000 - mae: 15101.1846 - val_loss: 383937728.0000 - val_mae: 19547.7695\n",
      "Epoch 62/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 273ms/step - loss: 229842464.0000 - mae: 14996.9316 - val_loss: 383903104.0000 - val_mae: 19546.8809\n",
      "Epoch 63/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 269ms/step - loss: 229742272.0000 - mae: 14996.1484 - val_loss: 383868416.0000 - val_mae: 19545.9961\n",
      "Epoch 64/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 273ms/step - loss: 231672240.0000 - mae: 15061.9307 - val_loss: 383833920.0000 - val_mae: 19545.1133\n",
      "Epoch 65/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 277ms/step - loss: 232531504.0000 - mae: 15082.0908 - val_loss: 383799392.0000 - val_mae: 19544.2305\n",
      "Epoch 66/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 277ms/step - loss: 231692176.0000 - mae: 15053.8926 - val_loss: 383764800.0000 - val_mae: 19543.3457\n",
      "Epoch 67/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 267ms/step - loss: 228918272.0000 - mae: 14974.0049 - val_loss: 383730080.0000 - val_mae: 19542.4570\n",
      "Epoch 68/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 268ms/step - loss: 225786704.0000 - mae: 14865.0576 - val_loss: 383695584.0000 - val_mae: 19541.5742\n",
      "Epoch 69/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 253ms/step - loss: 231422064.0000 - mae: 15049.7451 - val_loss: 383660960.0000 - val_mae: 19540.6875\n",
      "Epoch 70/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 257ms/step - loss: 230988512.0000 - mae: 15037.4600 - val_loss: 383626336.0000 - val_mae: 19539.8008\n",
      "Epoch 71/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 250ms/step - loss: 230894512.0000 - mae: 15036.6592 - val_loss: 383591904.0000 - val_mae: 19538.9219\n",
      "Epoch 72/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - loss: 234465504.0000 - mae: 15163.6006 - val_loss: 383557216.0000 - val_mae: 19538.0312\n",
      "Epoch 73/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 244ms/step - loss: 230315552.0000 - mae: 15018.1504 - val_loss: 383522688.0000 - val_mae: 19537.1484\n",
      "Epoch 74/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 243ms/step - loss: 228371680.0000 - mae: 14946.4902 - val_loss: 383488064.0000 - val_mae: 19536.2617\n",
      "Epoch 75/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 230ms/step - loss: 227157104.0000 - mae: 14905.4551 - val_loss: 383453472.0000 - val_mae: 19535.3789\n",
      "Epoch 76/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 243ms/step - loss: 232662176.0000 - mae: 15099.6602 - val_loss: 383418752.0000 - val_mae: 19534.4883\n",
      "Epoch 77/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 234ms/step - loss: 228270240.0000 - mae: 14945.2783 - val_loss: 383384192.0000 - val_mae: 19533.6055\n",
      "Epoch 78/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 230ms/step - loss: 229533024.0000 - mae: 14986.4805 - val_loss: 383349376.0000 - val_mae: 19532.7148\n",
      "Epoch 79/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 235ms/step - loss: 230358176.0000 - mae: 15014.9805 - val_loss: 383314528.0000 - val_mae: 19531.8203\n",
      "Epoch 80/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 232365040.0000 - mae: 15084.1943 - val_loss: 383279680.0000 - val_mae: 19530.9297\n",
      "Epoch 81/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 233574544.0000 - mae: 15121.9219 - val_loss: 383244928.0000 - val_mae: 19530.0410\n",
      "Epoch 82/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 245ms/step - loss: 231529712.0000 - mae: 15057.3496 - val_loss: 383210304.0000 - val_mae: 19529.1523\n",
      "Epoch 83/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 238ms/step - loss: 232654528.0000 - mae: 15093.4883 - val_loss: 383175712.0000 - val_mae: 19528.2695\n",
      "Epoch 84/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 253ms/step - loss: 228699104.0000 - mae: 14961.3975 - val_loss: 383141248.0000 - val_mae: 19527.3867\n",
      "Epoch 85/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 236ms/step - loss: 232512512.0000 - mae: 15092.0225 - val_loss: 383106400.0000 - val_mae: 19526.4922\n",
      "Epoch 86/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 240ms/step - loss: 231045488.0000 - mae: 15039.6416 - val_loss: 383071776.0000 - val_mae: 19525.6055\n",
      "Epoch 87/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 233ms/step - loss: 228855424.0000 - mae: 14963.8262 - val_loss: 383037312.0000 - val_mae: 19524.7227\n",
      "Epoch 88/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 241ms/step - loss: 231150800.0000 - mae: 15042.0137 - val_loss: 383002688.0000 - val_mae: 19523.8359\n",
      "Epoch 89/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 238ms/step - loss: 226513936.0000 - mae: 14889.2686 - val_loss: 382968512.0000 - val_mae: 19522.9629\n",
      "Epoch 90/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 239ms/step - loss: 230974880.0000 - mae: 15035.3076 - val_loss: 382933824.0000 - val_mae: 19522.0742\n",
      "Epoch 91/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 246ms/step - loss: 230405216.0000 - mae: 15022.3818 - val_loss: 382899232.0000 - val_mae: 19521.1875\n",
      "Epoch 92/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - loss: 231596112.0000 - mae: 15059.3047 - val_loss: 382864800.0000 - val_mae: 19520.3066\n",
      "Epoch 93/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 234ms/step - loss: 231832000.0000 - mae: 15069.5342 - val_loss: 382830432.0000 - val_mae: 19519.4258\n",
      "Epoch 94/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 230ms/step - loss: 230174112.0000 - mae: 15017.2520 - val_loss: 382796000.0000 - val_mae: 19518.5430\n",
      "Epoch 95/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - loss: 228262960.0000 - mae: 14944.1494 - val_loss: 382761536.0000 - val_mae: 19517.6602\n",
      "Epoch 96/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - loss: 230473664.0000 - mae: 15018.7676 - val_loss: 382727136.0000 - val_mae: 19516.7773\n",
      "Epoch 97/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - loss: 232818160.0000 - mae: 15100.3281 - val_loss: 382692416.0000 - val_mae: 19515.8906\n",
      "Epoch 98/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 234ms/step - loss: 229381536.0000 - mae: 14977.8926 - val_loss: 382658016.0000 - val_mae: 19515.0078\n",
      "Epoch 99/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 230ms/step - loss: 228826736.0000 - mae: 14964.4375 - val_loss: 382623616.0000 - val_mae: 19514.1250\n",
      "Epoch 100/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 238ms/step - loss: 227805280.0000 - mae: 14931.4199 - val_loss: 382589088.0000 - val_mae: 19513.2402\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 🔧 Compile first!\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 🛑 Add EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ✅ Now fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3535c03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be7d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"lstm_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0bf40c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit and save the scaler\u001b[39;00m\n\u001b[0;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m----> 6\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(X_train)  \u001b[38;5;66;03m# Replace with your actual training data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(scaler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Test loading it back\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Fit and save the scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)  # Replace with your actual training data\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# ✅ Test loading it back\n",
    "loaded_scaler = joblib.load(\"scaler.pkl\")\n",
    "print(type(loaded_scaler))  # Expected output: <class 'sklearn.preprocessing._data.MinMaxScaler'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882e6da5-c351-45d7-82e1-b3a293680cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dji_df=pd.read_csv(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\kaggle\\archive\\Processed_DJI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3c828c-b6f3-478f-a918-0237b518c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric or irrelevant columns\n",
    "dji_df = dji_df.drop(columns=  ['Date', 'Name'])\n",
    "\n",
    "# Predict next day's Close → shift -1\n",
    "dji_df['Target'] = dji_df['Close'].shift(-1)\n",
    "\n",
    "# Drop the last row (its target is NaN)\n",
    "dji_df = dji_df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = dji_df.drop(columns=['Target'])\n",
    "y = dji_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47f38c4-1b08-4f5d-9d2d-fc01c5da3800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02276028 0.67367022 0.35458231 ... 0.46262957 0.41979337 0.30124224]\n",
      " [0.03414128 0.67353689 0.65455703 ... 0.74086197 0.77759652 0.64596273]\n",
      " [0.03753761 0.67290954 0.56085971 ... 0.43480633 0.46764546 0.40062112]\n",
      " ...\n",
      " [0.9946794  0.67320111 0.53493206 ... 0.466994   0.78520935 0.64285714]\n",
      " [0.97055894 0.67847565 0.38127075 ... 0.53518822 0.6286025  0.60559006]\n",
      " [0.983683   0.67316734 0.62359749 ... 0.51009274 0.62425231 0.49378882]]\n",
      "[11107.969727 11146.570313 11132.55957  ... 21750.730469 21674.509766\n",
      " 21812.089844]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "\n",
    "\n",
    "# Convert to dataframe if needed\n",
    "import numpy as np\n",
    "X_scaled = np.array(X_scaled)\n",
    "y = np.array(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "667fb668-bd98-488e-842b-5d397490f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input shape: (1054, 60, 82)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 60\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(sequence_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-sequence_length:i])\n",
    "    y_seq.append(y[i])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)  # Should be (samples, 60, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680e0535-26b7-476e-bf9a-6b4277b9a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee4e957d-953d-4626-abec-2da1d4dceee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 277ms/step - loss: 233020336.0000 - mae: 15108.5957 - val_loss: 386121344.0000 - val_mae: 19603.5430\n",
      "Epoch 2/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 207ms/step - loss: 235408352.0000 - mae: 15184.4229 - val_loss: 386026912.0000 - val_mae: 19601.1348\n",
      "Epoch 3/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 209ms/step - loss: 234062544.0000 - mae: 15140.7998 - val_loss: 385980288.0000 - val_mae: 19599.9434\n",
      "Epoch 4/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 217ms/step - loss: 234870816.0000 - mae: 15166.9678 - val_loss: 385939872.0000 - val_mae: 19598.9121\n",
      "Epoch 5/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 230357728.0000 - mae: 15018.8545 - val_loss: 385901056.0000 - val_mae: 19597.9238\n",
      "Epoch 6/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 213ms/step - loss: 234738576.0000 - mae: 15163.5654 - val_loss: 385863360.0000 - val_mae: 19596.9590\n",
      "Epoch 7/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 230011712.0000 - mae: 15008.4756 - val_loss: 385826048.0000 - val_mae: 19596.0078\n",
      "Epoch 8/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 222ms/step - loss: 233765968.0000 - mae: 15124.8721 - val_loss: 385789120.0000 - val_mae: 19595.0664\n",
      "Epoch 9/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 229839584.0000 - mae: 14996.0654 - val_loss: 385752704.0000 - val_mae: 19594.1367\n",
      "Epoch 10/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 226545680.0000 - mae: 14879.3906 - val_loss: 385716480.0000 - val_mae: 19593.2148\n",
      "Epoch 11/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 208ms/step - loss: 231127728.0000 - mae: 15040.5527 - val_loss: 385680640.0000 - val_mae: 19592.2969\n",
      "Epoch 12/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - loss: 230320848.0000 - mae: 15016.1191 - val_loss: 385644928.0000 - val_mae: 19591.3867\n",
      "Epoch 13/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 206ms/step - loss: 231529728.0000 - mae: 15057.5000 - val_loss: 385609632.0000 - val_mae: 19590.4883\n",
      "Epoch 14/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 213ms/step - loss: 233345040.0000 - mae: 15114.7891 - val_loss: 385574112.0000 - val_mae: 19589.5801\n",
      "Epoch 15/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 206ms/step - loss: 231591152.0000 - mae: 15058.3682 - val_loss: 385538464.0000 - val_mae: 19588.6719\n",
      "Epoch 16/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 227210720.0000 - mae: 14909.3066 - val_loss: 385502848.0000 - val_mae: 19587.7617\n",
      "Epoch 17/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 208ms/step - loss: 234286240.0000 - mae: 15145.0244 - val_loss: 385467328.0000 - val_mae: 19586.8555\n",
      "Epoch 18/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 207ms/step - loss: 232085424.0000 - mae: 15076.2793 - val_loss: 385432096.0000 - val_mae: 19585.9531\n",
      "Epoch 19/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - loss: 232070944.0000 - mae: 15064.2061 - val_loss: 385396704.0000 - val_mae: 19585.0508\n",
      "Epoch 20/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 207ms/step - loss: 231620224.0000 - mae: 15065.7979 - val_loss: 385361472.0000 - val_mae: 19584.1523\n",
      "Epoch 21/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 234713840.0000 - mae: 15163.5469 - val_loss: 385326304.0000 - val_mae: 19583.2539\n",
      "Epoch 22/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 234294576.0000 - mae: 15148.6475 - val_loss: 385291040.0000 - val_mae: 19582.3535\n",
      "Epoch 23/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - loss: 233316848.0000 - mae: 15115.0488 - val_loss: 385256032.0000 - val_mae: 19581.4590\n",
      "Epoch 24/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 213ms/step - loss: 231189888.0000 - mae: 15050.7988 - val_loss: 385220832.0000 - val_mae: 19580.5625\n",
      "Epoch 25/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - loss: 232104496.0000 - mae: 15078.7080 - val_loss: 385185664.0000 - val_mae: 19579.6621\n",
      "Epoch 26/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 211ms/step - loss: 234452656.0000 - mae: 15152.0303 - val_loss: 385150496.0000 - val_mae: 19578.7656\n",
      "Epoch 27/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - loss: 228993376.0000 - mae: 14971.4316 - val_loss: 385115616.0000 - val_mae: 19577.8750\n",
      "Epoch 28/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - loss: 233363904.0000 - mae: 15118.4111 - val_loss: 385080448.0000 - val_mae: 19576.9746\n",
      "Epoch 29/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 211ms/step - loss: 232659072.0000 - mae: 15091.6738 - val_loss: 385045536.0000 - val_mae: 19576.0840\n",
      "Epoch 30/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 231022816.0000 - mae: 15035.2656 - val_loss: 385010592.0000 - val_mae: 19575.1914\n",
      "Epoch 31/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 210ms/step - loss: 229004560.0000 - mae: 14980.3379 - val_loss: 384975584.0000 - val_mae: 19574.2969\n",
      "Epoch 32/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - loss: 232934720.0000 - mae: 15105.9072 - val_loss: 384940640.0000 - val_mae: 19573.4062\n",
      "Epoch 33/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - loss: 231019792.0000 - mae: 15043.2305 - val_loss: 384905920.0000 - val_mae: 19572.5195\n",
      "Epoch 34/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 225ms/step - loss: 229776528.0000 - mae: 14997.8281 - val_loss: 384871392.0000 - val_mae: 19571.6367\n",
      "Epoch 35/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 232546288.0000 - mae: 15094.2354 - val_loss: 384836352.0000 - val_mae: 19570.7402\n",
      "Epoch 36/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 230347968.0000 - mae: 15009.0479 - val_loss: 384801600.0000 - val_mae: 19569.8535\n",
      "Epoch 37/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - loss: 234126736.0000 - mae: 15137.7393 - val_loss: 384766528.0000 - val_mae: 19568.9570\n",
      "Epoch 38/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 225ms/step - loss: 229744048.0000 - mae: 14997.2314 - val_loss: 384731744.0000 - val_mae: 19568.0664\n",
      "Epoch 39/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - loss: 232604592.0000 - mae: 15094.4805 - val_loss: 384696864.0000 - val_mae: 19567.1758\n",
      "Epoch 40/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 233668176.0000 - mae: 15123.8779 - val_loss: 384661920.0000 - val_mae: 19566.2852\n",
      "Epoch 41/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - loss: 230544160.0000 - mae: 15019.3447 - val_loss: 384627104.0000 - val_mae: 19565.3945\n",
      "Epoch 42/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 232384816.0000 - mae: 15089.3223 - val_loss: 384592256.0000 - val_mae: 19564.5039\n",
      "Epoch 43/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - loss: 228360880.0000 - mae: 14954.4229 - val_loss: 384557536.0000 - val_mae: 19563.6133\n",
      "Epoch 44/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 225ms/step - loss: 230909904.0000 - mae: 15035.3262 - val_loss: 384522784.0000 - val_mae: 19562.7266\n",
      "Epoch 45/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 231ms/step - loss: 230747888.0000 - mae: 15032.9248 - val_loss: 384488096.0000 - val_mae: 19561.8398\n",
      "Epoch 46/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 230961536.0000 - mae: 15035.8105 - val_loss: 384453472.0000 - val_mae: 19560.9570\n",
      "Epoch 47/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 230190784.0000 - mae: 15017.4229 - val_loss: 384418880.0000 - val_mae: 19560.0703\n",
      "Epoch 48/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 217ms/step - loss: 233860688.0000 - mae: 15132.3955 - val_loss: 384383968.0000 - val_mae: 19559.1777\n",
      "Epoch 49/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 231705584.0000 - mae: 15065.8984 - val_loss: 384349088.0000 - val_mae: 19558.2871\n",
      "Epoch 50/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 222ms/step - loss: 231648400.0000 - mae: 15059.4922 - val_loss: 384314368.0000 - val_mae: 19557.3984\n",
      "Epoch 51/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 228495328.0000 - mae: 14952.2959 - val_loss: 384279808.0000 - val_mae: 19556.5156\n",
      "Epoch 52/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 235557872.0000 - mae: 15188.4775 - val_loss: 384244960.0000 - val_mae: 19555.6250\n",
      "Epoch 53/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 219ms/step - loss: 234123040.0000 - mae: 15142.3809 - val_loss: 384210400.0000 - val_mae: 19554.7402\n",
      "Epoch 54/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - loss: 228139792.0000 - mae: 14944.4512 - val_loss: 384175872.0000 - val_mae: 19553.8594\n",
      "Epoch 55/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 233ms/step - loss: 233267888.0000 - mae: 15117.7207 - val_loss: 384141120.0000 - val_mae: 19552.9688\n",
      "Epoch 56/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 218ms/step - loss: 228846688.0000 - mae: 14959.2373 - val_loss: 384106080.0000 - val_mae: 19552.0742\n",
      "Epoch 57/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - loss: 234409408.0000 - mae: 15151.2393 - val_loss: 384071296.0000 - val_mae: 19551.1836\n",
      "Epoch 58/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 217ms/step - loss: 228359664.0000 - mae: 14950.8721 - val_loss: 384036768.0000 - val_mae: 19550.3008\n",
      "Epoch 59/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 231011936.0000 - mae: 15038.3828 - val_loss: 384002336.0000 - val_mae: 19549.4219\n",
      "Epoch 60/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 230182816.0000 - mae: 15011.5020 - val_loss: 383967648.0000 - val_mae: 19548.5352\n",
      "Epoch 61/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - loss: 232618816.0000 - mae: 15089.2588 - val_loss: 383932992.0000 - val_mae: 19547.6465\n",
      "Epoch 62/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 228217248.0000 - mae: 14944.7510 - val_loss: 383898240.0000 - val_mae: 19546.7578\n",
      "Epoch 63/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 229077424.0000 - mae: 14976.4023 - val_loss: 383863584.0000 - val_mae: 19545.8711\n",
      "Epoch 64/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 232594304.0000 - mae: 15095.4492 - val_loss: 383829248.0000 - val_mae: 19544.9922\n",
      "Epoch 65/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - loss: 234001568.0000 - mae: 15144.4463 - val_loss: 383794656.0000 - val_mae: 19544.1094\n",
      "Epoch 66/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 233243680.0000 - mae: 15113.5547 - val_loss: 383759872.0000 - val_mae: 19543.2207\n",
      "Epoch 67/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 230766512.0000 - mae: 15030.0303 - val_loss: 383725504.0000 - val_mae: 19542.3398\n",
      "Epoch 68/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 225ms/step - loss: 229333072.0000 - mae: 14981.9492 - val_loss: 383690912.0000 - val_mae: 19541.4531\n",
      "Epoch 69/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 231174112.0000 - mae: 15039.2705 - val_loss: 383656096.0000 - val_mae: 19540.5645\n",
      "Epoch 70/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 231751968.0000 - mae: 15064.7646 - val_loss: 383621504.0000 - val_mae: 19539.6777\n",
      "Epoch 71/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 229615248.0000 - mae: 14990.3799 - val_loss: 383587008.0000 - val_mae: 19538.7969\n",
      "Epoch 72/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 229196016.0000 - mae: 14983.3057 - val_loss: 383552288.0000 - val_mae: 19537.9082\n",
      "Epoch 73/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 231113424.0000 - mae: 15046.5283 - val_loss: 383517632.0000 - val_mae: 19537.0195\n",
      "Epoch 74/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - loss: 231362480.0000 - mae: 15053.6670 - val_loss: 383483008.0000 - val_mae: 19536.1348\n",
      "Epoch 75/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - loss: 230378704.0000 - mae: 15018.3105 - val_loss: 383448480.0000 - val_mae: 19535.2520\n",
      "Epoch 76/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - loss: 230022368.0000 - mae: 15002.4131 - val_loss: 383413952.0000 - val_mae: 19534.3652\n",
      "Epoch 77/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - loss: 231830000.0000 - mae: 15063.3418 - val_loss: 383379392.0000 - val_mae: 19533.4805\n",
      "Epoch 78/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 223ms/step - loss: 225717392.0000 - mae: 14863.2217 - val_loss: 383344896.0000 - val_mae: 19532.5977\n",
      "Epoch 79/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 228362848.0000 - mae: 14947.7246 - val_loss: 383310208.0000 - val_mae: 19531.7129\n",
      "Epoch 80/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - loss: 229878192.0000 - mae: 15003.1113 - val_loss: 383275520.0000 - val_mae: 19530.8242\n",
      "Epoch 81/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 231392000.0000 - mae: 15049.8252 - val_loss: 383240896.0000 - val_mae: 19529.9375\n",
      "Epoch 82/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - loss: 226642288.0000 - mae: 14895.4150 - val_loss: 383206496.0000 - val_mae: 19529.0566\n",
      "Epoch 83/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 218ms/step - loss: 228598416.0000 - mae: 14951.7715 - val_loss: 383171840.0000 - val_mae: 19528.1680\n",
      "Epoch 84/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 233266240.0000 - mae: 15112.0205 - val_loss: 383137472.0000 - val_mae: 19527.2871\n",
      "Epoch 85/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 226880736.0000 - mae: 14899.8184 - val_loss: 383103104.0000 - val_mae: 19526.4102\n",
      "Epoch 86/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - loss: 229171744.0000 - mae: 14976.3887 - val_loss: 383068864.0000 - val_mae: 19525.5312\n",
      "Epoch 87/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 225ms/step - loss: 229794688.0000 - mae: 15000.8916 - val_loss: 383034240.0000 - val_mae: 19524.6445\n",
      "Epoch 88/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - loss: 229442704.0000 - mae: 14988.9775 - val_loss: 382999552.0000 - val_mae: 19523.7559\n",
      "Epoch 89/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 231213040.0000 - mae: 15045.2373 - val_loss: 382964960.0000 - val_mae: 19522.8711\n",
      "Epoch 90/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - loss: 230925792.0000 - mae: 15034.9482 - val_loss: 382930368.0000 - val_mae: 19521.9863\n",
      "Epoch 91/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 223ms/step - loss: 229406576.0000 - mae: 14983.7861 - val_loss: 382895776.0000 - val_mae: 19521.0977\n",
      "Epoch 92/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - loss: 230307520.0000 - mae: 15015.2178 - val_loss: 382861312.0000 - val_mae: 19520.2148\n",
      "Epoch 93/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - loss: 231016752.0000 - mae: 15041.2236 - val_loss: 382826848.0000 - val_mae: 19519.3320\n",
      "Epoch 94/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 229513952.0000 - mae: 14997.0059 - val_loss: 382792352.0000 - val_mae: 19518.4492\n",
      "Epoch 95/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 229840912.0000 - mae: 14998.1787 - val_loss: 382757952.0000 - val_mae: 19517.5684\n",
      "Epoch 96/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 228823728.0000 - mae: 14968.6650 - val_loss: 382723520.0000 - val_mae: 19516.6875\n",
      "Epoch 97/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 230233552.0000 - mae: 15015.1436 - val_loss: 382689088.0000 - val_mae: 19515.8027\n",
      "Epoch 98/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 228607024.0000 - mae: 14963.4600 - val_loss: 382654624.0000 - val_mae: 19514.9219\n",
      "Epoch 99/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - loss: 229870704.0000 - mae: 15002.5303 - val_loss: 382620160.0000 - val_mae: 19514.0371\n",
      "Epoch 100/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 221ms/step - loss: 229644528.0000 - mae: 14999.2939 - val_loss: 382585664.0000 - val_mae: 19513.1523\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 🔧 Compile first!\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 🛑 Add EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ✅ Now fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "433c9828-6f4f-4622-9415-baeedb9c2c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aa2c3cf-6319-4c82-9133-ed1fc99663c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8e0d5fe-6373-4b7e-92c8-e88426ab6613",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit and save the scaler\u001b[39;00m\n\u001b[0;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m----> 6\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(X_train)  \u001b[38;5;66;03m# Replace with your actual training data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(scaler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Test loading it back\u001b[39;00m\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    491\u001b[0m     X,\n\u001b[0;32m    492\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[0;32m    493\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m    494\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    495\u001b[0m )\n\u001b[0;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\sklearn\\utils\\validation.py:1058\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Fit and save the scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)  # Replace with your actual training data\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# ✅ Test loading it back\n",
    "loaded_scaler = joblib.load(\"scaler.pkl\")\n",
    "print(type(loaded_scaler))  # Expected output: <class 'sklearn.preprocessing._data.MinMaxScaler'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d520dd4d-67c4-445e-a214-cfe2b1b72636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved!\n",
      "Scaler loaded: <class 'sklearn.preprocessing._data.MinMaxScaler'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Let's say X_train has shape (samples, timesteps, features)\n",
    "samples, timesteps, features = X_train.shape\n",
    "\n",
    "# Reshape to 2D (samples * timesteps, features)\n",
    "X_reshaped = X_train.reshape(-1, features)\n",
    "\n",
    "# Fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_reshaped)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"Scaler saved!\")\n",
    "\n",
    "# Optional: Test loading it\n",
    "loaded_scaler = joblib.load(\"scaler.pkl\")\n",
    "print(\"Scaler loaded:\", type(loaded_scaler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bcfac1c-9022-443f-ac18-1cb238c6d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_scaled = scaler.transform(X_reshaped).reshape(samples, timesteps, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "818cfabc-cc61-49a2-8178-36dc725a2ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"lstm_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb82593-99eb-4975-944e-409a71c4b568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
